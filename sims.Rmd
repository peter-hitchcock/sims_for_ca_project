---
title: "Sims of actor-critic vs q-learner"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
``` 

```{r}
sapply(c("stringr", "dplyr", "data.table", "purrr", "foreach", "doParallel", "ggplot2"), require, character=TRUE)
sf <- function() sapply(paste0("./Functions/", list.files("./Functions/", recursive=TRUE)), source) # Source all fxs
sf()
DefPlotPars()
```

Plan: 
- back to A-C next  

# Params to consider manipulating:  

- have transfer phase after every train phase?
- magnitude (like the SCZ studies)   
- probability, especially lucas-newman lab work and general idea worry = less precise prob est  
- possible that worry diffs relate just to (or more to) explicit rather than implicit learning and we should try to 
distinguish these  
  - could have more stimuli eg. 16 w matched rew probabilities but different amounts of reward  
    - and could have rews less predictable so harder to understand structure (eg. 1 3 4 5 7 -6)  
    - then at end have them try to identify probabilities and rewards explicitly  

```{r}
########################### Set Up Experiment ###########################
states <- c("ab", "cd", "ef", "gh")
every_stim <- c("a", "b", "c", "d", "e", "f", "g", "h")
better_state <- c("left", "left", "right", "right")
# Create a key to access every trial and output pos or neg reward with the appropriate probability  
key <- data.table("stim"=every_stim, 
                  "pos_or_neg"=c(1, 1, 1, 1, -1, -1, -1, -1) , 
                  "p_nz"=c(.9, .8, .2, .1, .9, .8, .2, .1)) # Probability of non-zero outcome
# To determine if made the optimal choice on a trial
state_key <- data.table(states, "better_choice"=better_state)
######################################################################

############ Create actor-critic simulation testbed  ##################
## Initializations ##
n_trials <- 400
helpers <- list() # List of stuff we'll need to shuttle around fxs 
params <- list() # Free pars
params[["beta"]] <- 100
params[["lapsiness"]] <- .025
params[["q_learner_prop"]] <- .9
params[["actor_LR"]] <- .1
params[["q_LR"]] <- .1
params[["critic_LR"]] <- .1
helpers[["params"]] <- params
helpers[["sim_or_opt"]] <- 1 # Simulate (1) or optimize (2)? (opt not yet built)
helpers[["n_trials"]] <- n_trials
training_trials <- unlist(lapply(1:n_trials, function(x) sample(states, 1))) # 1 training phase

# More settings
parallelize <- 1
if (parallelize) registerDoParallel(cores=round(detectCores()*2/3))
verbose <- 0 # Trial-wise print out?

# iters <- 80
# if (parallelize) {
#   out <- foreach(i=1:iters) %dopar% RunATrainPhase(states, key, state_key, helpers, verbose)   
# } else {
#   out <- foreach(i=1:iters) %do% RunATrainPhase(states, key, state_key, helpers, verbose)   
# }
# 
# for (iter in 1:iters) out[[iter]]$iter <- iter # Label iteration
# out_dt <- do.call(rbind, out)
```

Testing: 
- We want to see how much we can get from N trials of training phase data in differentiating pts with different reliance on actor-critic. 
  - As a first pass, look at raw accuracy data just at training for 100 trials with a range of q-learner (vs. AC) reliance  


To do, when back: 
- Set params above to realistic values basd on papers  
- Implement decay?  

```{r}
q_learner_prop_range <- seq(.1, 1, .1)
ql_train_test <- lapply(q_learner_prop_range, function(x) {
  # Simulate q_learner at a given range #
  params[["q_learner_prop"]] <- x
  helpers[["params"]] <- params
  helpers[["n_trials"]] <- 400
  iters <- 40
  if (parallelize) {
    verbose <- 0
    out <- foreach(i=1:iters) %dopar% RunATrainPhase(states, key, state_key, helpers, verbose)   
  } else {
    out <- foreach(i=1:iters) %do% RunATrainPhase(states, key, state_key, helpers, verbose)   
  }
  
  for (iter in 1:iters) out[[iter]]$iter <- iter # Label iteration
  out_dt <- do.call(rbind, out)
  out_dt$q_learner_prop <- x # Store q-learner prop
out_dt
})
ql_train_dt <- ql_train_test %>% bind_rows()
```


```{r}
# Not improving over time so still seems to be a bug
summs <- ql_train_dt %>% group_by(tidx, q_learner_prop, group=1) %>% summarize(m=mean(correct))

# Looks like you only get much difference asymptotically with 400 trials
ggplot(summs, aes(x=tidx, y=m)) + geom_line(size=.8) + 
  ga + ap + geom_hline(yintercept=c(.5, .6, .7, .8, .9)) + facet_wrap(~ q_learner_prop) 
```
# Simming risky choice tasks  - All code now in sims.R

From supplement Konova, risk tolerance is around 1 on average exponentiated. The parameter is 
$$EV^{risk}$$  
Given the even odds of choices in Rutledge, this'll produce even EV with risk=1 and the gain offer at twice the risk offer  

Plan: 
- Test ability to recover risk vs. beta parameters 
  - Then consider alternate specs of task (eg. 25/50/75 probs a la Konova)
  - And possibly add in ambiguous risk 
  

## Parameter recovery for task variants  

Variant 1 

Exact variant from Rutledge et al. 14 modeled using risk tol approach from Konova et al. 19 
but w/o modeling ambiguity since that's not present 

Results: Complete inability to recover beta. 
- Next step: vary probability to see if that improves ability to identify unknown risk and in turn beta  
```{r}
## Variants 
variant_1 <- 
  read.table("./../../data/model_res/dev_sim_param_recov/pr_tv1", sep=",", header=TRUE)
```

```{r}
ggplot(variant_1, aes(x=beta_sim, y=beta_opt)) + 
  geom_point(size=4, pch=21, fill="gray57") + ga + ap
ggplot(variant_1, aes(x=risk_tol_sim, y=risk_tol_opt)) + 
  geom_point(size=4, pch=21, fill="gray57") + ga + ap
```

